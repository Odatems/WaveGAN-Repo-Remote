{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e067e8c-0e1d-469b-994b-56610522d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the required packages \n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import pprint as pp\n",
    "import os\n",
    "from numpy.random import default_rng\n",
    "from utils.graph_utils import *\n",
    "from Gurobi_tsp_reader import GurobiTSPReader\n",
    "from logging import root\n",
    "from utils.model_utils import *\n",
    "import networkx as nx\n",
    "\n",
    "from models.Descriminator import Critic\n",
    "from models.Generator import Generator\n",
    "from optim import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31185123-bf2f-46f1-a028-32a9b0629b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(mse_val,status):\n",
    "    \n",
    "    # plot the mse of the validation over epochs \n",
    "    plt.plot(mse_va, linewidth = 2)\n",
    "    plt.title('MSE over iterations during' + status, fontsize = 14)\n",
    "    plt.xlabel('Epoch number',fontsize = 14)\n",
    "    plt.ylabel('MSE',fontsize = 14)\n",
    "    xticks(np.arange(0, mse_val.shape[0], step=1))  # Set label locations.\n",
    "    plt.xticks(np.arange(0, mse_val.shape[0], step=1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b269b21c-1756-451a-8bd1-4a87161cee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions.\n",
    "# create models\n",
    "# load the datasets \n",
    "# Train the model \n",
    "# Test the model\n",
    "# Other helper functions \n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def get_gradient(crit, real, fake, epsilon):\n",
    "\n",
    "    mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "    mixed_scores = crit(mixed_images)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=mixed_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores), \n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return gradient\n",
    "\n",
    "def gradient_penalty(gradient):\n",
    "\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    return penalty\n",
    "\n",
    "def get_gen_loss(crit_fake_pred, fake, real,g_on_real_pred,epoch_num):\n",
    "    \n",
    "    lambda_recon = 200\n",
    "    recon_criterion =    nn.L1Loss() \n",
    "    gen_rec_loss = recon_criterion(real, fake)\n",
    "    \n",
    "    recon_criterion_iden =  nn.L1Loss() \n",
    "    identity_loss = recon_criterion_iden(real, g_on_real_pred)\n",
    "    gen_loss = -1. * torch.mean(crit_fake_pred) + (lambda_recon) * gen_rec_loss+ identity_loss\n",
    "    \n",
    "    return gen_loss\n",
    "\n",
    "\n",
    "def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n",
    "    crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp \n",
    "    return crit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e61d4-51ed-4d54-9436-9a18a3cddf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_filepath,val_filepath,num_nodes = 20,train_dataset_size = 1e10, valid_dataset_size_all = 10000):\n",
    "    \n",
    "    \n",
    "    num_neighbors= -1\n",
    "    batch_size= 1\n",
    "    # Read training dataset \n",
    "    train_dataset = GurobiTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n",
    "    xx = np.zeros((train_dataset_size ,1,num_nodes,num_nodes)) # \n",
    "    z_norm = np.zeros((train_dataset_size,1,num_nodes,num_nodes)) #\n",
    "    optimal_tour_len_train = np.zeros((train_dataset_size,1))\n",
    "    optimal_tour_nodes_train = np.zeros((train_dataset_size,num_nodes))\n",
    "    i = iter(train_dataset)\n",
    "    for itr_num in range(np.int32(train_dataset_size)):\n",
    "        \n",
    "        next_batch = next(i)\n",
    "        xx[itr_num] = next_batch.edges_target\n",
    "        z_norm[itr_num] = next_batch.edges_values\n",
    "        # Get the optimal tour\n",
    "        optimal_tour_len_train[itr_num]=next_batch.tour_len # Get the optimal tour\n",
    "        optimal_tour_nodes_train[itr_num] = next_batch.tour_nodes  # Get the tour nodes \n",
    "    \n",
    "    # plot matrix for testing validity \n",
    "    plt.imshow(xx[0,0,:,:])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(z_norm[0,0,:,:])\n",
    "    plt.show()\n",
    "    \n",
    "    # Read validation dataset\n",
    "    val_dataset = GurobiTSPReader(num_nodes, num_neighbors, batch_size, val_filepath)\n",
    "    validation_set_sample = np.zeros((valid_dataset_size_all,1,num_nodes,num_nodes)) # \n",
    "    z_norm_valid = np.zeros((valid_dataset_size_all,1,num_nodes,num_nodes)) # \n",
    "    optimal_tour_len_val = np.zeros((valid_dataset_size_all,1))\n",
    "    optimal_tour_nodes_val = np.zeros((valid_dataset_size_all,num_nodes))\n",
    "    \n",
    "    i = iter(val_dataset)\n",
    "    for valid_itr_num in range(np.int32(valid_dataset_size_all)):#\n",
    "        \n",
    "        \n",
    "        next_batch = next(i)\n",
    "        validation_set_sample[valid_itr_num] = next_batch.edges_target\n",
    "        z_norm_valid[valid_itr_num] = next_batch.edges_values\n",
    "        \n",
    "        # Get the optimal tour\n",
    "        optimal_tour_len_val[valid_itr_num]=next_batch.tour_len # Get the optimal tour\n",
    "        optimal_tour_nodes_val[valid_itr_num] = next_batch.tour_nodes  # Get the tour nodes \n",
    "        \n",
    "    return xx,z_norm,validation_set_sample,z_norm_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c24026-5bbc-44e3-b65b-5c78901e8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_nodes\", type=int, default=20)\n",
    "    parser.add_argument(\"--train_filepath\", type=int, default=None)\n",
    "    parser.add_argument(\"--val_filepath\", type=str, default=None)\n",
    "    parser.add_argument(\"--test_filepath\", type=str, default=None)\n",
    "    parser.add_argument(\"--train_dataset_size\",type=int,default=1e6)\n",
    "    parser.add_argument(\"--valid_dataset_size\",type=int,default=1e4)\n",
    "    parse.add_argument(\"--testing_datset_size\", type=int,default = 1e4)\n",
    "    parser.add_argument(\"--load_best_train\", type = bool,default=True)\n",
    "    parser.add_argument(\"--load_best_test\", type = bool,default=True)\n",
    "    parser.add_argument(\"--pretrained\",type=bool,default=False)\n",
    "    parser.add_argument(\"--n_epochs\",type=int,default=100)\n",
    "    parser.add_argument(\"--beam_size\",type=int,default=1024)\n",
    "    \n",
    "    opts = parser.parse_args()\n",
    "    # if the filee names are not specified\n",
    "    if train_filepath ==None:\n",
    "        train_filepath = f\"mmwave{num_nodes}_Gurobi_multi_proc.txt\"\n",
    "    \n",
    "    if val_filepath == None:\n",
    "        val_filepath = f\"mmwave{num_nodes}_val_Gurobi_multi_proc.txt\"\n",
    "        \n",
    "    if test_filepath ==  None:\n",
    "        test_filepath = f\"mmwave{num_nodes}_test_Gurobi_multi_proc.txt\"\n",
    "        \n",
    "        \n",
    "    display_step = 1000\n",
    "    lr = 0.0002\n",
    "    beta_1 = 0.5\n",
    "    beta_2 = 0.999\n",
    "    c_lambda = 10\n",
    "    crit_repeats = 5\n",
    "    device = 'cuda'\n",
    "    val_mse = []\n",
    "    \n",
    "    \n",
    "    gen = Generator().to(device) \n",
    "    crit = Critic().to(device) \n",
    "    \n",
    "    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "    crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "    \n",
    "    gen = gen.apply(weights_init)\n",
    "    crit = crit.apply(weights_init)\n",
    "    with open('Model_results_summary.txt',\"a\" , encoding=\"utf-8\") as f:\n",
    "        \n",
    "        f.write('Model Parameters')\n",
    "        f.write(\"Number of epochs = \" + str(n_epochs))\n",
    "        f.write(\"Pretrained = \" + str(pretrained))\n",
    "        f.write(\"Pre-trained with best = \" + str(load_best_train))\n",
    "        f.write(\"Tested with best = \" + str(load_best_test))\n",
    "       \n",
    "        \n",
    "    gen,val_mse = train_model(gen,crit,train_filepath,val_filepath,train_dataset_size,valid_dataset_size,n_epochs,pretrained,load_best_train)\n",
    "    model_testing(gen,num_nodes, testing_datset_size, beam_size,test_filepath,load_best_test)\n",
    "    status = 'train'\n",
    "    plot_model_results(np.asarray(val_mse,status)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9143f47-4196-4d9a-97d7-ba765c270d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
