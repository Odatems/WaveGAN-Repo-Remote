# WaveGAN 
## Overview


## Gurobi Dataset Generation 
1. **To generate dataset using Gurobi, run any of the following commands based on the objective:**
      ```
      cd /Data/mmWave Data/Gurobi_based_data<br>
      python Gurobi_mmwave_data_generator.py --num_nodes {Number of Nodes} --num_samples {dataset size} --node_dim {3d coordinates} --num_cores {Number of the cores for multiprocessing} --max_h {maximum height} --max_y {y-limit of the deployment region} --max_x {x-limit of the deployment region} --fun_type {}
     ```
     
     </br>
 Where: --fun_type is 1 for **sum-rate** objective and 2 for **log-sum** objective. The generated file is "Gurobi_mmwave{num_nodes}_sumrate_obj_core{core_id}.txt" or "Gurobi_mmwave{num_nodes}_logsum_obj_core{core_id}.txt"
</br>


2.    **To split the generated dataset, run the following command:**
      ```
      python Gurobi_data_splitter.py --num_nodes {Number of Nodes} --val_size {validation dataset size} --test_size {testing dataset size} --filename {file to split}
      ```
      </br>

The generated files are: Gurobi_mmwave{num_nodes}_val_data.txt, Gurobi_mmwave{num_nodes}_test_data.txt, and Gurobi_mmwave{num_nodes}_train_data.txt
</br>

3.    **To combine datasets generated by different cores, use the following command:**

      ```
       python Gurobi_datasets_combiner.py --num_cores {number of cores} --num_nodes {Number of nodes} --base_filename {the first file in the list to be combined} --filename {file name that aggregates all sub-datasets} 
       ```
</br>
The name of the file generated is inserted using --filename or by default "Gurobi_mmwave{num_nodes}_sumrate_obj.txt". The base file is the first file in the list to read from. By default it is "Gurobi_mmwave{num_nodes}_sumrate_obj_core0.txt"


## Matrix-Based Concorde Dataset Generation
The package used to generate using Concorde is a modified version of [PyConcorde](https://github.com/jvkersch/pyconcorde.git). The code include the capability to solve the problem based on cost matricies or nodes coordinates. 


</br>

To generate datasets using Concorde, run the following commands to install pyConcorde: </br>

```
cd /Data/mmWave_Data/Concorde_based_data
git clone https://github.com/jvkersch/pyconcorde

```
Next, replace **tsp.py** and **_concorde.pyx** in pyconcorde/concorde with same files in /Data/mmWave_Data/Concorde_based_data. Then execute the following commands: 

```
cd pyconcorde
pip install -e .
cd ..
```
Run the data generation script:

```
python Concorde_mmwave_dataset_generator.py --num_nodes {Number of nodes} --num_cores {Number of cores on the machine} --node_dim {dimension of the coordinates} --num_samples {dataset size} --max_h {maximum height} --max_y {y-limit of the deployment region} --max_x {x-limit of the deployment region} --fun_type {} --file_name {the file name that contains the generated dataset}
```

## To train the model, for example for 20 nodes, run the folowing script:
```
python mmWaveGAN_20_nodes.py --num_nodes {num_nodes} --train_filepath {training dataset} --val_filepath {validation dataset} --test_filepath {testing dataset}  --train_dataset_size {The size of the training dataset} --valid_dataset_size {size of the validation dataset} --testing_datset_size {size of the testing dataset} --load_best_train {True: if the best trainig weights are loaded} --load_best_test {True: if the model is to be tested based on the best weights} --pretrained {True: if the model initialization is based on previously trained model} --n_epochs {Number of training epochs} --beam_siz {beam size used in the beam search step}
```

Note that the data reader used inside the code depends on the type of the solver used to gerneate the data. 
